{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "128c7a53",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# The Physics Team: An Implementation of **H**amiltonian **M**onte **C**arlo (HMC)\n",
    "\n",
    "**Allison Colarelli**, **Jiyun Di**, **IvÃ¡n Espinoza Bustamante**, **Heidi Liu**, and **Sebastian Sage**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a928bbc4",
   "metadata": {},
   "source": [
    "#  A useful demo/animation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b13c86c-3d49-46a6-b17a-58001bea758a",
   "metadata": {},
   "source": [
    "The Markov-chain Monte Carlo Interactive Gallery: https://chi-feng.github.io/mcmc-demo/app.html?algorithm=HamiltonianMC&target=banana"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bb6d6d-f954-4a9a-bc05-f2f06cd77389",
   "metadata": {},
   "source": [
    "# Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0337310-ea7e-4c28-bb63-d40492706c1e",
   "metadata": {},
   "source": [
    "## Background: **M**arkov **C**hain **M**onte **C**arlo (MCMC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5df8fea",
   "metadata": {
    "id": "b5df8fea"
   },
   "source": [
    "Oftentimes, probability distributions are difficult to express or normalize analytically. However, it is still possible to sample over the entire distribution given the ratio of the probabilities between two points. Starting at some initial point $\\vec{q_i}$, we can randomly draw a new value $\\vec{q_{try}}$ and define this ratio as the probability to accept $\\vec{q_{try}}$ as the new starting point:\n",
    "$$P_{\\rm accept} = \\min\\left(1, \\frac{P(\\vec{q_{\\rm try}})}{P(\\vec{q_{i}})}\\right)$$\n",
    "\n",
    "The most basic method of implementing this is the Metropolis-Hastings (MH) algorithm. To get $\\vec{q_{try}}$, the MH algorithm simply draws from a Gaussian with a mean of $\\vec{q_i}$. This algorithm, though simple, works very well on problems where $\\vec{q}$ has a small number of dimensions. However, as $\\vec{q}$ grows in size, the chance that $P(\\vec{q_{\\rm try}}) > P(\\vec{q_{i}})$ gets very small. As a result, $P_{\\rm accept}$ drops to nearly zero, and the algorithm takes a very long time to converge. To fix this, we must modify our method of drawing $\\vec{q_{try}}$ to keep $P_{\\rm accept}$ as large as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cb3feb-f3ec-42c1-8b4c-45000eed894a",
   "metadata": {},
   "source": [
    "## **H**amiltonian **M**onte **C**arlo (HMC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fb9716",
   "metadata": {
    "id": "a9fb9716"
   },
   "source": [
    "Let's start with the crazy first step of doubling the number of parameters we have, and sampling from $P(\\vec{q}, \\vec{p})=P(\\vec{q})P(\\vec{p})$. The key idea here is that we can choose $\\vec{p}$ to be independent of $\\vec{q}$, and we can choose what $P(\\vec{p})$ looks like. In most cases, we will choose a Gaussian.\n",
    "$$P(q,p) = P(q)e^{-\\frac{p^2}{2m}}$$\n",
    "\n",
    "We can suggestively rewrite this as follows so that we can think in terms of a Hamiltonian $H$\n",
    "\n",
    "$$P(q,p) = e^{-\\left(\\frac{p^2}{2m} - \\ln(P(q))\\right)}$$\n",
    "\n",
    "$$H(q,p) = \\frac{p^2}{2m} + U(q), \\space U(q) = -\\ln{P(q)}$$\n",
    "\n",
    "Now, if we find a way to get a new set of variables $p_{try}$ and $q_{try}$ that have the same energy, we will be guaranteed to accept the new set of variables. Luckily, Classical Mechanics comes to the rescue with Hamilton's Equations:\n",
    "\n",
    "$$\\frac{dq}{dt}=\\frac{\\partial H}{\\partial p}, \\space \\frac{dp}{dt}=-\\frac{\\partial H}{\\partial q}$$\n",
    "\n",
    "Now, how will our algorithm actually work? If we begin with the initial position $q_i$, then $p_i$ will be selected with a Gaussian random draw with a mean of zero and variance equal to some mass $m$. $q_{\\rm try}$ and $p_{\\rm try}$ can then be obtained by numerically integrating Hamilton's equations. If we do this perfectly, we are guaranteed to accept our step and can store $q_{\\rm try}$.\n",
    "\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53120840",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --no-cache-dir git+https://github.com/ua-2025q3-astr501-513/p1-the-physics-team.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf6b7df-95ec-4acd-af91-8c90754843af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy     as np\n",
    "import jax.numpy as jnp\n",
    "from   jax           import grad, random, vmap\n",
    "from   tqdm.notebook import tqdm\n",
    "import hmc\n",
    "\n",
    "from getdist import MCSamples, plots\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "names = [r'$x$',r'$y$']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63de9d4-ea29-4dce-9067-65338725cc27",
   "metadata": {},
   "source": [
    "### Define the Kinetic and Potential Energies\n",
    "\n",
    "$$ T(p) = \\frac{p^2}{2m}, \\quad U(q) = -\\ln P(q) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b25cc1-679e-4095-b6e6-d0cfca7c1a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Potential(q, L):\n",
    "    \"\"\"\n",
    "    Compute the potential energy U(q) = -ln(L(q)) in JAX function form.\n",
    "    Parameters\n",
    "    ----------\n",
    "    q : array-like\n",
    "        Position, in parameter space.\n",
    "    L : callable\n",
    "        Function of a probability distribution P(q) related to Hamiltonian H(q, p).\n",
    "        This is what we want to sample from. We hoped to use \"P\" for parameter\n",
    "        name, but this will cause ambiguity with the Potential name.\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Negative log-potential energy at position q.\n",
    "    \"\"\"\n",
    "    return -jnp.log(L(q))\n",
    "\n",
    "def Kinetic(p, minv):\n",
    "    \"\"\"\n",
    "    Compute the kinetic energy T(p) = 0.5 * (p^T M^{-1} p) in JAX function form.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    p : array-like\n",
    "        Momentum vector.\n",
    "    mass : array-like\n",
    "        Mass matrix. We already considered higher-dimensional momentum \n",
    "\t\tvectors. So that the mass is also a matrix, where only the diagonal \n",
    "\t\tentries are non-zero.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Kinetic energy corresponding to momentum p.\n",
    "    \"\"\"\n",
    "    return 0.5 * p @ minv @ p   # = 0.5 * (p^T M^{-1} p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b1732d-dc2b-44ad-994c-883e117b5a81",
   "metadata": {},
   "source": [
    "## Leapfrog Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb21d35-5d06-41c7-9538-f1d877d74127",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ua-2025q3-astr501-513/p1-the-physics-team/main/doc/HMC-leapfrog-cropped.gif\" width=\"420\" align=\"right\">\n",
    "\n",
    "It should be noted that integrators do not perfectly conserve energy, so a symplectic integrator must be carefully selected to prevent any systematic drift in energy. We specifically choose to implement the Leapfrog algorithm.\n",
    "\n",
    "For a timestep $\\Delta t$, begin by evaluating the momentum $p(t)$ at the next half-step $t+\\frac{\\Delta t}{2}$:\n",
    "$$p_{n+1/2} = p_{n-1/2} - \\frac{\\Delta t}{2} \\frac{\\partial U}{\\partial q}|_{q_n}$$\n",
    "\n",
    "Then utilize this to evaluate position at integer timesteps:\n",
    "\n",
    "$$q_{n+1} = q_n + \\frac{1}{m}p_{n+1/2} \\Delta t$$\n",
    "\n",
    "The process then repeats for the desired number of steps, with $p$ updated each half-step and $q$ updated each integer step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18de57c-e5f0-4642-93ce-94b6b59c4024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Leapfrog(q0, p0, dt, Nsteps, L, Massinv):\n",
    "    \"\"\"\n",
    "    A leapfrog integrator solving for Hamiltonian (H) in the kick-drift-kick scheme.\n",
    "\tThis was introduced in Lecture 9 (Mon, Sep 29, 2025):\n",
    "\thttps://ua-2025q3-astr501-513.github.io/notes-9/#leapfrog-verlet-integrator\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    q0 : array-like\n",
    "        Initial position.\n",
    "    p0 : array-like\n",
    "        Initial momentum.\n",
    "    dt : float\n",
    "        Time size for every step.\n",
    "    Nsteps : int\n",
    "        Number of leapfrog total integration steps.\n",
    "    L : callable\n",
    "\t\tFunction of a probability distribution P(q) related to Hamiltonian H(q, p).\n",
    "    Mass : array-like\n",
    "        Mass matrix.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (array, array)\n",
    "        (Position, momentum) tuple (q, p) giving the new position and momentum after integration.\n",
    "    \"\"\"\n",
    "    q = q0\n",
    "    dUdq = grad(Potential, argnums=0)\n",
    "\t\n",
    "    # Half-step momentum update\n",
    "    p = p0 - 0.5 * dt * dUdq(q, L) # Half-step\n",
    "    \n",
    "    # Full steps\n",
    "    for _ in range(Nsteps - 1):\n",
    "        q = q + dt * Massinv @ p      # Full-step\n",
    "        p = p - dt * dUdq(q, L)    # Full-step \n",
    "    \n",
    "    # Final position and half momentum update\n",
    "    q = q + dt * Massinv @ p          # Final full-step\n",
    "    p = p - 0.5 * dt * dUdq(q, L)  # Final half-step\n",
    "    \n",
    "    return q, p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdeecaf-0202-4417-9cf9-b100c8d76c0a",
   "metadata": {},
   "source": [
    "## Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6265408b-31ad-4278-9cdc-053c9ee707f4",
   "metadata": {},
   "source": [
    "At the start of each iteration, a random momentum vector is drawn from a Gaussian distribution $p \\sim \\mathcal{N}(0, M)$, where M is the mass matrix. The current state ($q_i$, $p_i$) is then evolved through a series of Leapfrog steps to produce a proposal ($q_{\\text{try}}$, $p_{\\text{try}}$). The potential energy $U(q) = -\\ln P(q)$ and kinetic energy $T(p) = \\frac{1}{2} p^T M^{-1} p$ are evaluated before and after the integration to compute the total Hamiltonian $H = T+U$.\n",
    "\n",
    "Because numerical integration does not perfectly conserve energy, the acceptance criterion is used to correct for small integration errors. The acceptance probability is given by\n",
    "\n",
    "$$P_{\\text{accept}} = \\min(1, e^{-\\Delta H}),$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\\Delta H = H_{\\text{final}} - H_{\\text{initial}}.$$\n",
    "\n",
    "If the proposed state is accepted, the sampler moves to $q_{\\text{try}}$; otherwise, it remains at $q_i$. This allows HMC to explore high-dimensional probability distributions efficiently, avoiding the slow random-walk behavior typical of traditional MCMC methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e334fc5-f24e-4745-bb62-293ea64deea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sampler(q0, dt, Nsteps, L, Mass, Massinv, rng_key):\n",
    "    \"\"\"\n",
    "    HMC sampler using leapfrog integrator.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    q0 : array-like\n",
    "        Initial position.\n",
    "    dt : float\n",
    "        Time size for every step.\n",
    "    Nsteps : int\n",
    "        Number of leapfrog total integration steps.\n",
    "    L : callable\n",
    "        Likelihood function.\n",
    "    Mass : array-like\n",
    "        Mass matrix.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    array-like\n",
    "        New sample position after Metropolis acceptance test.\n",
    "    \"\"\"\n",
    "    # Draw a random momentum vector from the Normal distribution: p ~ N(0, Mass)\n",
    "    key1, key2 = random.split(rng_key)\n",
    "    p0 = random.multivariate_normal(key1, jnp.zeros_like(q0), Mass)\n",
    "\n",
    "\t# Compute new (q, p) after given N steps from the leapfrog integration\n",
    "    q, p = Leapfrog(q0, p0, dt, Nsteps, L, Massinv)\n",
    "\n",
    "    # Compute initial and final energies\n",
    "\t# Reason: In fact, in numerical calculation, we cannot compute the true path of (q, p)\n",
    "\t#         with the constant Hamiltonian/energy. Check what the difference is below. \n",
    "    Uinit  = Potential(q0, L)\n",
    "    Ufinal = Potential(q, L)\n",
    "    Kinit  = Kinetic(p0, Massinv)\n",
    "    Kfinal = Kinetic(p, Massinv)\n",
    "\n",
    "    # Metropolis acceptance criterion\n",
    "    # Reason: If ideally, our computed (q_new, p_new) has the same energy, we are\n",
    "    #         very happy to accept this (q_new, p_new). Otherwise (also in most cases), \n",
    "    #         we still accept it but with a likelihood of ~ min(1, e^{-ÎH}).\n",
    "    accept_prob = jnp.exp(Uinit - Ufinal + Kinit - Kfinal)\n",
    "    u = random.uniform(key2)\n",
    "\n",
    "    return jnp.where(u < accept_prob, q, q0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9e49d5-2998-4776-9f4a-c0ed07052275",
   "metadata": {},
   "source": [
    "## HMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7061c3e8-ad3b-4a32-9c0c-19d5d9c9ec11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hmc(q0, Nsamples, dt, Nsteps, L, Mass, burnin=0, rng_key=None):\n",
    "    \"\"\"\n",
    "    Main Hamiltonian Monte Carlo (HMC) sampling.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    q0 : array-like\n",
    "        Initial position (a parameter vector).\n",
    "    Nsamples : int\n",
    "        Number of samples.\n",
    "    dt : float\n",
    "        Time size for every leapfrog integration step.\n",
    "    Nsteps : int\n",
    "        Number of leapfrog steps per sample.\n",
    "    L : callable\n",
    "        Likelihood distribution function of position/parameter.\n",
    "    Mass : array-like\n",
    "        Mass matrix.\n",
    "    burnin : int, optional\n",
    "        Number of initial samples to discard. Default is 0.\n",
    "    \n",
    "    rng_key: array-like, optional\n",
    "        PRNG key (an array of seeds) to be used in random sampling. Defaults to a PRNG key with seed = 0.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Array of accepted samples after burn-in.\n",
    "    \"\"\"\n",
    "\n",
    "    if rng_key is None:\n",
    "        rng_key = random.PRNGKey(0)\n",
    "\n",
    "    # Set first q_current to be initial positions q0\n",
    "    q_current = q0\n",
    "    samples = []\n",
    "\n",
    "    # Compute inverse of mass matrix\n",
    "    minv = jnp.linalg.inv(Mass) # = M^{-1}\n",
    "\n",
    "    # Run the sampler\n",
    "    for _ in tqdm(range(Nsamples + burnin)):\n",
    "        # Generate new key\n",
    "        rng_key, subkey = random.split(rng_key)\n",
    "\n",
    "        # Update q_current based upon the previous q_current, using the method outlined in Sampler\n",
    "        q_current = Sampler(q_current, dt, Nsteps, L, Mass, minv, subkey)\n",
    "        samples.append(q_current)\n",
    "    \n",
    "    # Remove burn-in samples\n",
    "    return jnp.array(samples[burnin:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b735c8ed",
   "metadata": {},
   "source": [
    "### - Gaussian 1D Distribution\n",
    "$$p(x; \\mu, \\sigma) = \\exp\\!\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9889214a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex. Gaussian 1D\n",
    "def Gaussian1D(q, mu=0.0, sigma=1.0):\n",
    "    \"\"\"\n",
    "    A 1D Gaussian (normal) probability distribution.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    q : array-like, shape (1,) or scalar\n",
    "        Position in 1D space.\n",
    "    mu : float\n",
    "        Mean of the distribution.\n",
    "    sigma : float\n",
    "        Standard deviation of the distribution.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Unnormalized probability density at position q.\n",
    "    \"\"\"\n",
    "    # Extract scalar if q is an array\n",
    "    x = q[0] if hasattr(q, '__len__') else q\n",
    "    \n",
    "    # Gaussian probability density (unnormalized is fine for HMC)\n",
    "    prob = jnp.exp(-0.5 * ((x - mu) / sigma)**2)\n",
    "    \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3c5044",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(42)\n",
    "q0 = jnp.array([2.0])\n",
    "samples = Hmc(q0=q0, Nsamples=1000, dt=0.5, Nsteps=2, L=Gaussian1D, Mass=jnp.eye(1), burnin=20, rng_key=key)\n",
    "# ==== Plot expected vs. samps\n",
    "x_vals = np.linspace(-4, 4, 100)\n",
    "y_vals = np.array([Gaussian1D(jnp.array([x]), mu=0.0, sigma=1.0) for x in x_vals])/np.sqrt(2*np.pi)\n",
    "\n",
    "plt.plot(x_vals, y_vals, label='Gaussian PDF',color='black',linestyle='dashed')\n",
    "plt.hist(samples.ravel(), density=True, label='HMC samples',bins=40)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5029fb81-e894-466b-8db1-e2a9bdf901c1",
   "metadata": {},
   "source": [
    "## Premium HMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4fa39e-7d6b-462f-9dd6-e0800267c7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hmc_Vectorized(q0_array, Nsamples, dt, Nsteps, L, Mass, burnin=0, rng_key=None):\n",
    "    \"\"\"\n",
    "    Vectorized Hamiltonian Monte Carlo (HMC) sampling.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    q0 : array-like\n",
    "        Initial positions of all chains.\n",
    "    Nsamples : int\n",
    "        Number of samples.\n",
    "    dt : float\n",
    "        Time size for every leapfrog integration step.\n",
    "    Nsteps : int\n",
    "        Number of leapfrog steps per sample.\n",
    "    L : callable\n",
    "        Likelihood distribution function of position/parameter.\n",
    "    Mass : array-like\n",
    "        Mass matrix.\n",
    "    burnin : int, optional\n",
    "        Number of initial samples to discard. Default is 0.\n",
    "    \n",
    "    rng_key: array-like, optional\n",
    "        PRNG key (an array of seeds) to be used in random sampling. Defaults to a PRNG key with seed = 0.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Array of accepted samples after burn-in.\n",
    "    \"\"\"\n",
    "\n",
    "    if rng_key is None:\n",
    "        rng_key = random.PRNGKey(0)\n",
    "\n",
    "    # Detect the number of chains to use based on the shape of q0\n",
    "    Nchains = q0_array.shape[0]\n",
    "    minv = jnp.linalg.inv(Mass) # = M^{-1}\n",
    "\n",
    "    # Vectorize the Sampler function across chains\n",
    "    # Uses optimized C code to run Sampler on multiple chains at once\n",
    "    sampler_vectorized = vmap(\n",
    "        lambda q, key: Sampler(q, dt, Nsteps, L, Mass, minv, key),\n",
    "        in_axes=(0, 0)\n",
    "    )\n",
    "\n",
    "    q_current = q0_array\n",
    "    samples = []\n",
    "\n",
    "    # Progress bar tracks total samples across all chains.\n",
    "    # Total number of evaluations shown is the closest number to Nsamples\n",
    "    # that is evenly divisible by Nchains\n",
    "    pbar = tqdm(total=(Nsamples//Nchains) * Nchains, desc=\"HMC sampling\")\n",
    "\n",
    "    for i in range(Nsamples//Nchains + burnin):\n",
    "        # Generate one key per chain\n",
    "        rng_key, *subkeys = random.split(rng_key, Nchains + 1)\n",
    "        subkeys = jnp.array(subkeys)\n",
    "\n",
    "        # Update all chains in parallel\n",
    "        q_current = sampler_vectorized(q_current, subkeys)\n",
    "        samples.append(q_current)\n",
    "\n",
    "        # Only update the progress bar after the burn-in has been completed\n",
    "        if i >= burnin:\n",
    "            pbar.update(Nchains)\n",
    "    \n",
    "    pbar.close()\n",
    "\n",
    "    # Remove burn-in samples and convert to array\n",
    "    samples = jnp.array(samples[burnin:])\n",
    "\n",
    "    # Reshape to (Nsamples, Ndim)\n",
    "    return samples.reshape(-1, samples.shape[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfc1f58-e84c-4152-85f8-b108af36b470",
   "metadata": {},
   "source": [
    "# Testing with Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6964c78",
   "metadata": {},
   "source": [
    "## - Gaussian 1D Distribution\n",
    "$$p(x; \\mu, \\sigma) = \\exp\\!\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1113078",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(42)\n",
    "q0_array = random.normal(key, (16, 1))\n",
    "\n",
    "samples = Hmc_Vectorized(q0_array=q0_array, Nsamples=5000, \n",
    "                         dt=0.5, Nsteps=2, L=Gaussian1D, \n",
    "                         Mass=jnp.eye(1), burnin=20, rng_key=key)\n",
    "\n",
    "# ==== Plot expected vs. samps\n",
    "x_vals = np.linspace(-4, 4, 100)\n",
    "y_vals = np.array([Gaussian1D(jnp.array([x]), mu=0.0, sigma=1.0) for x in x_vals])/np.sqrt(2*np.pi)\n",
    "\n",
    "plt.plot(x_vals, y_vals, label='Gaussian PDF',color='black',linestyle='dashed')\n",
    "plt.hist(samples.ravel(), density=True, label='HMC samples',bins=60)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b78717f-0450-42ca-9b4a-a1126312d874",
   "metadata": {},
   "source": [
    "## - Correlated 2D Gaussian Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312ccfb7-a0d8-4768-b7d1-6d2abdf97830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gaussian(q):\n",
    "    #cov = jnp.array([[1, 0.5], [0.5, 1]])\n",
    "    covinv = jnp.array([[4./3., -2./3.],[-2./3., 4./3.]])\n",
    "    return jnp.exp(-0.5 * q @ covinv @ q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0463807-5cc9-4f9b-adf6-d74eb1b2c3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(42)\n",
    "q0_array = random.normal(key, (16, 2))  # 16 chains starting from different points\n",
    "\n",
    "samples = Hmc_Vectorized(q0_array, Nsamples=5000, dt=0.5, Nsteps=2, \n",
    "                         L=Gaussian, Mass=jnp.eye(2), burnin=10, rng_key=key)\n",
    "\n",
    "hmc_samps = MCSamples(samples=samples, names=names, label=\"HMC\")\n",
    "\n",
    "# ==== Comparison with truth\n",
    "true_samples = np.random.multivariate_normal(np.zeros(2),jnp.array([[1, 0.5], [0.5, 1]]),5000)\n",
    "true_samps = MCSamples(samples=true_samples, names=names,label=\"Truth\")\n",
    "# ====\n",
    "g = plots.get_subplot_plotter()\n",
    "g.triangle_plot([true_samps, hmc_samps], filled=True, smooth_scale_1D=1., smooth_scale_2D=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e50e011-8e8d-4b5a-81fd-b2de3f23935f",
   "metadata": {},
   "source": [
    "## - Donut Distribution \n",
    "\n",
    "$$P(x,y) = \\exp\\left(-0.5 \\frac{|\\sqrt{(x^2+y^2)}-R|}{r}\\right)$$\n",
    "\n",
    "This one might take a bit longer to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa242ca-6c57-4ba7-a268-63b938424573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Donut(q, R=2.0, r=0.5):\n",
    "    \"\"\"\n",
    "    A donut (torus) shaped 2D probability distribution.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    q : array-like, shape (2,)\n",
    "        Position [x, y] in 2D space.\n",
    "    R : float\n",
    "        Radius from origin to center of donut tube (major radius).\n",
    "    r : float\n",
    "        Radius of the donut tube itself (minor radius).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Unnormalized probability density at position q.\n",
    "    \"\"\"\n",
    "    x, y = q[0], q[1]\n",
    "    \n",
    "    # Distance from origin\n",
    "    distance_from_origin = jnp.sqrt(x**2 + y**2)\n",
    "    \n",
    "    # Distance from the donut's center ring\n",
    "    distance_from_ring = jnp.abs(distance_from_origin - R)\n",
    "    \n",
    "    # Gaussian falloff from the ring\n",
    "    # Higher density near the ring (distance_from_ring â 0)\n",
    "    # Lower density away from the ring\n",
    "    sigma = r\n",
    "    prob = jnp.exp(-0.5 * (distance_from_ring / sigma)**2)\n",
    "    \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c8950b-df94-4a97-9409-1aacdc9e8908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex. Donut\n",
    "key = random.PRNGKey(42)\n",
    "q0_array = random.normal(key, (16, 2))  # 16 independent chains\n",
    "\n",
    "mass = jnp.eye(2)\n",
    "names = ['x', 'y']\n",
    "\n",
    "# === Run Vectorized HMC ===\n",
    "samples = Hmc_Vectorized(q0_array=q0_array, Nsamples=5000, \n",
    "                         dt=0.5, Nsteps=6, L=Donut, \n",
    "                         Mass=mass, burnin=20, rng_key=key)\n",
    "hmc_samps = MCSamples(samples=samples, names=names, label=\"HMC (Donut)\")\n",
    "\n",
    "# === Plot ===\n",
    "g = plots.get_subplot_plotter()\n",
    "g.triangle_plot([hmc_samps], filled=True, smooth_scale_1D=1.0, smooth_scale_2D=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76980a5-dfba-45fe-a0bd-b73560ecf751",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Other distributions we have tested\n",
    "\n",
    "### Banana Distribution: \n",
    "\n",
    "#### $P(x,y) = \\exp{\\left(-\\frac{(1-x)^2+100(y-x^2)^2}{200}\\right)}$\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/ua-2025q3-astr501-513/p1-the-physics-team/main/doc/banana.jpg\" width=\"450\">\n",
    "\n",
    "### Gamma Distribution: \n",
    "\n",
    "#### $p(x; k, \\theta) = \\begin{cases} \\dfrac{1}{\\Gamma(k)\\,\\theta^k}\\, x^{k-1} e^{-x / \\theta}, & x > 0, \\\\ 0, & x \\le 0, \\end{cases}$\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/ua-2025q3-astr501-513/p1-the-physics-team/main/doc/gamma.jpg\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3588d6f-d460-4f8d-ac45-b111604d493d",
   "metadata": {},
   "source": [
    "# Hands-on: Some example distributions you can try on your own\n",
    "\n",
    "#### - *Log-Normal Distribution*: $p(x \\mid \\mu, \\sigma^2, \\lambda) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\frac{1}{x + \\lambda} \\exp\\left( -\\frac{1}{2\\sigma^2} \\left[ \\ln(x + \\lambda) - \\mu \\right]^2 \\right)$\n",
    "\n",
    "#### - *Exponential Distribution*: $p(x; \\lambda) = \\lambda e^{-\\lambda x}$\n",
    "\n",
    "#### - *Rayleigh Distribution*: $p(x; \\sigma) = \\frac{x}{\\sigma^2} e^{-x^2 / (2\\sigma^2)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82650d8b-7eb4-4a28-8016-945782fd06ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: try your choice of distribution\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8418a2f-23ff-4b76-b51e-c631a2412729",
   "metadata": {},
   "source": [
    "# Papers and documentation\n",
    "\n",
    "*   **Data Analysis Recipes: Using Markov Chain Monte Carlo:**  https://arxiv.org/pdf/1710.06068\n",
    "*   **JAX Documentation:** https://docs.jax.dev/en/latest/index.html\n",
    "*   **Conceptual Introduction to HMC:** https://arxiv.org/pdf/1701.02434\n",
    "*   **MCMC using Hamiltonian Dynamics:** https://arxiv.org/pdf/1206.1901"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668cb4a2-efd8-4b41-bf6b-9c3f347cc656",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
